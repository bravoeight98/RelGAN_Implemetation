{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RelGAN_master_implemetation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bravoeight98/RelGAN_Implemetation/blob/main/RelGAN_master_implemetation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMfZRMS0q1TF"
      },
      "source": [
        "Preprocesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjgu83cvlVjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5638f8-48ea-424e-e6e9-2c8a3b476baa"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-n\", \"--number\", type=int, default=17, help=\"attribute number\")\n",
        "parser.add_argument(\"-o\", \"--output\", type=str, default='anno_dic.npy', help=\"output file\")\n",
        "parser.add_argument(\"-f\", \"--file\", required=False) \n",
        "args = parser.parse_args()\n",
        "\n",
        "annos = open('list_attr_celeba.txt').readlines()\n",
        "\n",
        "attrs = str.split(annos[1])\n",
        "print(attrs)\n",
        "\n",
        "if args.number == 17:\n",
        "    new_attrs = ['5_o_Clock_Shadow', 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Male', 'Mustache', 'Pale_Skin', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Hat', 'Young']\n",
        "elif  args.number == 9:\n",
        "    new_attrs = ['Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Mustache', 'Pale_Skin', 'Smiling', 'Young']\n",
        "elif  args.number == 5:\n",
        "    new_attrs = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']\n",
        "else:\n",
        "    print('You can only choose 17, 9, 5  combination')\n",
        "    exit()\n",
        "\n",
        "new_attrs_index = []\n",
        "for x in new_attrs:\n",
        "    new_attrs_index.append(attrs.index(x))\n",
        "print(new_attrs_index)\n",
        "\n",
        "annosAry = {}\n",
        "for i in range(2,len(annos)):\n",
        "    anno = str.split(annos[i])\n",
        "    temp = [(int(i)+1)/2 for i in anno[1:]]\n",
        "    temp2 = []\n",
        "    for ii in new_attrs_index:\n",
        "        temp2.append(temp[ii])\n",
        "    annosAry[anno[0]] = temp2\n",
        "    \n",
        "print(annosAry[\"000001.jpg\"])\n",
        "print(len(annosAry[\"000001.jpg\"]))\n",
        "\n",
        "np.save(args.output, annosAry)\n",
        "\n",
        "img_list = open('image_list.txt').readlines()\n",
        "imgIndex = [None]*len(img_list)\n",
        "\n",
        "for i in range(1,len(img_list)):\n",
        "    temp = str.split(img_list[i])\n",
        "    imgIndex[int(temp[0])] = temp[2]\n",
        "    \n",
        "print(imgIndex[29999])\n",
        "\n",
        "np.save(\"imgIndex.npy\", imgIndex)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
            "[0, 4, 5, 8, 9, 11, 15, 16, 17, 20, 22, 26, 31, 32, 33, 35, 39]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]\n",
            "17\n",
            "116055.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8L1Q8Ymy3-H"
      },
      "source": [
        "Contrib.ops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ju9UeMpy69Y"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from keras.engine import Layer, InputSpec\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras import backend as K\n",
        "\n",
        "class GroupNormalization(Layer):\n",
        "    \"\"\"Group normalization layer\n",
        "    Group Normalization divides the channels into groups and computes within each group\n",
        "    the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
        "    and its accuracy is stable in a wide range of batch sizes\n",
        "    # Arguments\n",
        "        groups: Integer, the number of groups for Group Normalization.\n",
        "        axis: Integer, the axis that should be normalized\n",
        "            (typically the features axis).\n",
        "            For instance, after a `Conv2D` layer with\n",
        "            `data_format=\"channels_first\"`,\n",
        "            set `axis=1` in `BatchNormalization`.\n",
        "        epsilon: Small float added to variance to avoid dividing by zero.\n",
        "        center: If True, add offset of `beta` to normalized tensor.\n",
        "            If False, `beta` is ignored.\n",
        "        scale: If True, multiply by `gamma`.\n",
        "            If False, `gamma` is not used.\n",
        "            When the next layer is linear (also e.g. `nn.relu`),\n",
        "            this can be disabled since the scaling\n",
        "            will be done by the next layer.\n",
        "        beta_initializer: Initializer for the beta weight.\n",
        "        gamma_initializer: Initializer for the gamma weight.\n",
        "        beta_regularizer: Optional regularizer for the beta weight.\n",
        "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
        "        beta_constraint: Optional constraint for the beta weight.\n",
        "        gamma_constraint: Optional constraint for the gamma weight.\n",
        "    # Input shape\n",
        "        Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a model.\n",
        "    # Output shape\n",
        "        Same shape as input.\n",
        "    # References\n",
        "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 groups=32,\n",
        "                 axis=-1,\n",
        "                 epsilon=1e-5,\n",
        "                 center=True,\n",
        "                 scale=True,\n",
        "                 beta_initializer='zeros',\n",
        "                 gamma_initializer='ones',\n",
        "                 beta_regularizer=None,\n",
        "                 gamma_regularizer=None,\n",
        "                 beta_constraint=None,\n",
        "                 gamma_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(GroupNormalization, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.groups = groups\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[self.axis]\n",
        "\n",
        "        if dim is None:\n",
        "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
        "                             'input tensor should have a defined dimension '\n",
        "                             'but the layer received an input with shape ' +\n",
        "                             str(input_shape) + '.')\n",
        "\n",
        "        if dim < self.groups:\n",
        "            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
        "                             'more than the number of channels (' +\n",
        "                             str(dim) + ').')\n",
        "\n",
        "        if dim % self.groups != 0:\n",
        "            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
        "                             'multiple of the number of channels (' +\n",
        "                             str(dim) + ').')\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
        "                                    axes={self.axis: dim})\n",
        "        shape = (dim,)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(shape=shape,\n",
        "                                         name='gamma',\n",
        "                                         initializer=self.gamma_initializer,\n",
        "                                         regularizer=self.gamma_regularizer,\n",
        "                                         constraint=self.gamma_constraint)\n",
        "        else:\n",
        "            self.gamma = None\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(shape=shape,\n",
        "                                        name='beta',\n",
        "                                        initializer=self.beta_initializer,\n",
        "                                        regularizer=self.beta_regularizer,\n",
        "                                        constraint=self.beta_constraint)\n",
        "        else:\n",
        "            self.beta = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        tensor_input_shape = K.shape(inputs)\n",
        "\n",
        "        # Prepare broadcasting shape.\n",
        "        reduction_axes = list(range(len(input_shape)))\n",
        "        del reduction_axes[self.axis]\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
        "        broadcast_shape.insert(1, self.groups)\n",
        "\n",
        "        reshape_group_shape = K.shape(inputs)\n",
        "        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
        "        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
        "        group_axes.insert(1, self.groups)\n",
        "\n",
        "        # reshape inputs to new group shape\n",
        "        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
        "        group_shape = K.stack(group_shape)\n",
        "        inputs = K.reshape(inputs, group_shape)\n",
        "\n",
        "        group_reduction_axes = list(range(len(group_axes)))\n",
        "        group_reduction_axes = group_reduction_axes[2:]\n",
        "\n",
        "        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
        "        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
        "\n",
        "        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
        "\n",
        "        # prepare broadcast shape\n",
        "        inputs = K.reshape(inputs, group_shape)\n",
        "        outputs = inputs\n",
        "\n",
        "        # In this case we must explicitly broadcast all parameters.\n",
        "        if self.scale:\n",
        "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "            outputs = outputs * broadcast_gamma\n",
        "\n",
        "        if self.center:\n",
        "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "            outputs = outputs + broadcast_beta\n",
        "\n",
        "        outputs = K.reshape(outputs, tensor_input_shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'groups': self.groups,\n",
        "            'axis': self.axis,\n",
        "            'epsilon': self.epsilon,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
        "        }\n",
        "        base_config = super(GroupNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "    \n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "class SwitchNormalization(Layer):\n",
        "    \"\"\"Switchable Normalization layer\n",
        "    Switch Normalization performs Instance Normalization, Layer Normalization and Batch\n",
        "    Normalization using its parameters, and then weighs them using learned parameters to\n",
        "    allow different levels of interaction of the 3 normalization schemes for each layer.\n",
        "    Only supports the moving average variant from the paper, since the `batch average`\n",
        "    scheme requires dynamic graph execution to compute the mean and variance of several\n",
        "    batches at runtime.\n",
        "    # Arguments\n",
        "        axis: Integer, the axis that should be normalized\n",
        "            (typically the features axis).\n",
        "            For instance, after a `Conv2D` layer with\n",
        "            `data_format=\"channels_first\"`,\n",
        "            set `axis=1` in `BatchNormalization`.\n",
        "        momentum: Momentum for the moving mean and the moving variance. The original\n",
        "            implementation suggests a default momentum of `0.997`, however it is highly\n",
        "            unstable and training can fail after a few epochs. To stabilise training, use\n",
        "            lower values of momentum such as `0.99` or `0.98`.\n",
        "        epsilon: Small float added to variance to avoid dividing by zero.\n",
        "        final_gamma: Bool value to determine if this layer is the final\n",
        "            normalization layer for the residual block.  Overrides the initialization\n",
        "            of the scaling weights to be `zeros`. Only used for Residual Networks,\n",
        "            to make the forward/backward signal initially propagated through an\n",
        "            identity shortcut.\n",
        "        center: If True, add offset of `beta` to normalized tensor.\n",
        "            If False, `beta` is ignored.\n",
        "        scale: If True, multiply by `gamma`.\n",
        "            If False, `gamma` is not used.\n",
        "            When the next layer is linear (also e.g. `nn.relu`),\n",
        "            this can be disabled since the scaling\n",
        "            will be done by the next layer.\n",
        "        beta_initializer: Initializer for the beta weight.\n",
        "        gamma_initializer: Initializer for the gamma weight.\n",
        "        mean_weights_initializer: Initializer for the mean weights.\n",
        "        variance_weights_initializer: Initializer for the variance weights.\n",
        "        moving_mean_initializer: Initializer for the moving mean.\n",
        "        moving_variance_initializer: Initializer for the moving variance.\n",
        "        beta_regularizer: Optional regularizer for the beta weight.\n",
        "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
        "        mean_weights_regularizer: Optional regularizer for the mean weights.\n",
        "        variance_weights_regularizer: Optional regularizer for the variance weights.\n",
        "        beta_constraint: Optional constraint for the beta weight.\n",
        "        gamma_constraint: Optional constraint for the gamma weight.\n",
        "        mean_weights_constraints: Optional constraint for the mean weights.\n",
        "        variance_weights_constraints: Optional constraint for the variance weights.\n",
        "    # Input shape\n",
        "        Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a model.\n",
        "    # Output shape\n",
        "        Same shape as input.\n",
        "    # References\n",
        "        - [Differentiable Learning-to-Normalize via Switchable Normalization](https://arxiv.org/abs/1806.10779)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 axis=-1,\n",
        "                 momentum=0.9,\n",
        "                 epsilon=1e-5,\n",
        "                 final_gamma=False,\n",
        "                 center=True,\n",
        "                 scale=True,\n",
        "                 beta_initializer='zeros',\n",
        "                 gamma_initializer='ones',\n",
        "                 mean_weights_initializer='ones',\n",
        "                 variance_weights_initializer='ones',\n",
        "                 moving_mean_initializer='ones',\n",
        "                 moving_variance_initializer='zeros',\n",
        "                 beta_regularizer=None,\n",
        "                 gamma_regularizer=None,\n",
        "                 mean_weights_regularizer=None,\n",
        "                 variance_weights_regularizer=None,\n",
        "                 beta_constraint=None,\n",
        "                 gamma_constraint=None,\n",
        "                 mean_weights_constraints=None,\n",
        "                 variance_weights_constraints=None,\n",
        "                 **kwargs):\n",
        "        super(SwitchNormalization, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.axis = axis\n",
        "        self.momentum = momentum\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        if final_gamma:\n",
        "            self.gamma_initializer = initializers.get('zeros')\n",
        "        else:\n",
        "            self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.mean_weights_initializer = initializers.get(mean_weights_initializer)\n",
        "        self.variance_weights_initializer = initializers.get(variance_weights_initializer)\n",
        "        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n",
        "        self.moving_variance_initializer = initializers.get(moving_variance_initializer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.mean_weights_regularizer = regularizers.get(mean_weights_regularizer)\n",
        "        self.variance_weights_regularizer = regularizers.get(variance_weights_regularizer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "        self.mean_weights_constraints = constraints.get(mean_weights_constraints)\n",
        "        self.variance_weights_constraints = constraints.get(variance_weights_constraints)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[self.axis]\n",
        "\n",
        "        if dim is None:\n",
        "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
        "                             'input tensor should have a defined dimension '\n",
        "                             'but the layer received an input with shape ' +\n",
        "                             str(input_shape) + '.')\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
        "                                    axes={self.axis: dim})\n",
        "        shape = (dim,)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(\n",
        "                shape=shape,\n",
        "                name='gamma',\n",
        "                initializer=self.gamma_initializer,\n",
        "                regularizer=self.gamma_regularizer,\n",
        "                constraint=self.gamma_constraint)\n",
        "        else:\n",
        "            self.gamma = None\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(\n",
        "                shape=shape,\n",
        "                name='beta',\n",
        "                initializer=self.beta_initializer,\n",
        "                regularizer=self.beta_regularizer,\n",
        "                constraint=self.beta_constraint)\n",
        "        else:\n",
        "            self.beta = None\n",
        "\n",
        "        self.moving_mean = self.add_weight(\n",
        "            shape=shape,\n",
        "            name='moving_mean',\n",
        "            initializer=self.moving_mean_initializer,\n",
        "            trainable=False)\n",
        "\n",
        "        self.moving_variance = self.add_weight(\n",
        "            shape=shape,\n",
        "            name='moving_variance',\n",
        "            initializer=self.moving_variance_initializer,\n",
        "            trainable=False)\n",
        "\n",
        "        self.mean_weights = self.add_weight(\n",
        "            shape=(3,),\n",
        "            name='mean_weights',\n",
        "            initializer=self.mean_weights_initializer,\n",
        "            regularizer=self.mean_weights_regularizer,\n",
        "            constraint=self.mean_weights_constraints)\n",
        "\n",
        "        self.variance_weights = self.add_weight(\n",
        "            shape=(3,),\n",
        "            name='variance_weights',\n",
        "            initializer=self.variance_weights_initializer,\n",
        "            regularizer=self.variance_weights_regularizer,\n",
        "            constraint=self.variance_weights_constraints)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "\n",
        "        # Prepare broadcasting shape.\n",
        "        reduction_axes = list(range(len(input_shape)))\n",
        "        del reduction_axes[self.axis]\n",
        "\n",
        "        if self.axis != 0:\n",
        "            del reduction_axes[0]\n",
        "\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        mean_instance = K.mean(inputs, reduction_axes, keepdims=True)\n",
        "        variance_instance = K.var(inputs, reduction_axes, keepdims=True)\n",
        "\n",
        "        mean_layer = K.mean(mean_instance, self.axis, keepdims=True)\n",
        "        temp = variance_instance + K.square(mean_instance)\n",
        "        variance_layer = K.mean(temp, self.axis, keepdims=True) - K.square(mean_layer)\n",
        "\n",
        "        def training_phase():\n",
        "            mean_batch = K.mean(mean_instance, axis=0, keepdims=True)\n",
        "            variance_batch = K.mean(temp, axis=0, keepdims=True) - K.square(mean_batch)\n",
        "\n",
        "            mean_batch_reshaped = K.flatten(mean_batch)\n",
        "            variance_batch_reshaped = K.flatten(variance_batch)\n",
        "\n",
        "            if K.backend() != 'cntk':\n",
        "                sample_size = K.prod([K.shape(inputs)[axis]\n",
        "                                      for axis in reduction_axes])\n",
        "                sample_size = K.cast(sample_size, dtype=K.dtype(inputs))\n",
        "\n",
        "                # sample variance - unbiased estimator of population variance\n",
        "                variance_batch_reshaped *= sample_size / (sample_size - (1.0 + self.epsilon))\n",
        "\n",
        "            self.add_update([K.moving_average_update(self.moving_mean,\n",
        "                                                     mean_batch_reshaped,\n",
        "                                                     self.momentum),\n",
        "                             K.moving_average_update(self.moving_variance,\n",
        "                                                     variance_batch_reshaped,\n",
        "                                                     self.momentum)],\n",
        "                            inputs)\n",
        "\n",
        "            return normalize_func(mean_batch, variance_batch)\n",
        "\n",
        "        def inference_phase():\n",
        "            mean_batch = self.moving_mean\n",
        "            variance_batch = self.moving_variance\n",
        "\n",
        "            return normalize_func(mean_batch, variance_batch)\n",
        "\n",
        "        def normalize_func(mean_batch, variance_batch):\n",
        "            mean_batch = K.reshape(mean_batch, broadcast_shape)\n",
        "            variance_batch = K.reshape(variance_batch, broadcast_shape)\n",
        "\n",
        "            mean_weights = K.softmax(self.mean_weights, axis=0)\n",
        "            variance_weights = K.softmax(self.variance_weights, axis=0)\n",
        "\n",
        "            mean = (mean_weights[0] * mean_instance +\n",
        "                    mean_weights[1] * mean_layer +\n",
        "                    mean_weights[2] * mean_batch)\n",
        "\n",
        "            variance = (variance_weights[0] * variance_instance +\n",
        "                        variance_weights[1] * variance_layer +\n",
        "                        variance_weights[2] * variance_batch)\n",
        "\n",
        "            outputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
        "\n",
        "            if self.scale:\n",
        "                broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "                outputs = outputs * broadcast_gamma\n",
        "\n",
        "            if self.center:\n",
        "                broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "                outputs = outputs + broadcast_beta\n",
        "\n",
        "            return outputs\n",
        "\n",
        "        if training in {0, False}:\n",
        "            return inference_phase()\n",
        "\n",
        "        return K.in_train_phase(training_phase,\n",
        "                                inference_phase,\n",
        "                                training=training)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'axis': self.axis,\n",
        "            'epsilon': self.epsilon,\n",
        "            'momentum': self.momentum,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'mean_weights_initializer': initializers.serialize(self.mean_weights_initializer),\n",
        "            'variance_weights_initializer': initializers.serialize(self.variance_weights_initializer),\n",
        "            'moving_mean_initializer': initializers.serialize(self.moving_mean_initializer),\n",
        "            'moving_variance_initializer': initializers.serialize(self.moving_variance_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'mean_weights_regularizer': regularizers.serialize(self.mean_weights_regularizer),\n",
        "            'variance_weights_regularizer': regularizers.serialize(self.variance_weights_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint),\n",
        "            'mean_weights_constraints': constraints.serialize(self.mean_weights_constraints),\n",
        "            'variance_weights_constraints': constraints.serialize(self.variance_weights_constraints),\n",
        "        }\n",
        "        base_config = super(SwitchNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "    \n",
        "class InstanceNormalization(Layer):\n",
        "    \"\"\"Instance normalization layer.\n",
        "    Normalize the activations of the previous layer at each step,\n",
        "    i.e. applies a transformation that maintains the mean activation\n",
        "    close to 0 and the activation standard deviation close to 1.\n",
        "    # Arguments\n",
        "        axis: Integer, the axis that should be normalized\n",
        "            (typically the features axis).\n",
        "            For instance, after a `Conv2D` layer with\n",
        "            `data_format=\"channels_first\"`,\n",
        "            set `axis=1` in `InstanceNormalization`.\n",
        "            Setting `axis=None` will normalize all values in each\n",
        "            instance of the batch.\n",
        "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
        "        epsilon: Small float added to variance to avoid dividing by zero.\n",
        "        center: If True, add offset of `beta` to normalized tensor.\n",
        "            If False, `beta` is ignored.\n",
        "        scale: If True, multiply by `gamma`.\n",
        "            If False, `gamma` is not used.\n",
        "            When the next layer is linear (also e.g. `nn.relu`),\n",
        "            this can be disabled since the scaling\n",
        "            will be done by the next layer.\n",
        "        beta_initializer: Initializer for the beta weight.\n",
        "        gamma_initializer: Initializer for the gamma weight.\n",
        "        beta_regularizer: Optional regularizer for the beta weight.\n",
        "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
        "        beta_constraint: Optional constraint for the beta weight.\n",
        "        gamma_constraint: Optional constraint for the gamma weight.\n",
        "    # Input shape\n",
        "        Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a Sequential model.\n",
        "    # Output shape\n",
        "        Same shape as input.\n",
        "    # References\n",
        "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
        "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
        "        https://arxiv.org/abs/1607.08022)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 axis=None,\n",
        "                 epsilon=1e-3,\n",
        "                 center=True,\n",
        "                 scale=True,\n",
        "                 beta_initializer='zeros',\n",
        "                 gamma_initializer='ones',\n",
        "                 beta_regularizer=None,\n",
        "                 gamma_regularizer=None,\n",
        "                 beta_constraint=None,\n",
        "                 gamma_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(InstanceNormalization, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        ndim = len(input_shape)\n",
        "        if self.axis == 0:\n",
        "            raise ValueError('Axis cannot be zero')\n",
        "\n",
        "        if (self.axis is not None) and (ndim == 2):\n",
        "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=ndim)\n",
        "\n",
        "        if self.axis is None:\n",
        "            shape = (1,)\n",
        "        else:\n",
        "            shape = (input_shape[self.axis],)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(shape=shape,\n",
        "                                         name='gamma',\n",
        "                                         initializer=self.gamma_initializer,\n",
        "                                         regularizer=self.gamma_regularizer,\n",
        "                                         constraint=self.gamma_constraint)\n",
        "        else:\n",
        "            self.gamma = None\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(shape=shape,\n",
        "                                        name='beta',\n",
        "                                        initializer=self.beta_initializer,\n",
        "                                        regularizer=self.beta_regularizer,\n",
        "                                        constraint=self.beta_constraint)\n",
        "        else:\n",
        "            self.beta = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        reduction_axes = list(range(0, len(input_shape)))\n",
        "\n",
        "        if self.axis is not None:\n",
        "            del reduction_axes[self.axis]\n",
        "\n",
        "        del reduction_axes[0]\n",
        "\n",
        "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
        "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
        "        normed = (inputs - mean) / stddev\n",
        "\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        if self.axis is not None:\n",
        "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        if self.scale:\n",
        "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "            normed = normed * broadcast_gamma\n",
        "        if self.center:\n",
        "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "            normed = normed + broadcast_beta\n",
        "        return normed\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'axis': self.axis,\n",
        "            'epsilon': self.epsilon,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
        "        }\n",
        "        base_config = super(InstanceNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNsx6RIqyUgH"
      },
      "source": [
        "Ops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxcjB6Nw7j8Y"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import LeakyReLU, Conv2D, Add, ZeroPadding2D, Activation, Lambda, Dropout\n",
        "from keras import backend as K\n",
        "\n",
        "def hard_tanh(x):\n",
        "    return K.clip(x, -1, )\n",
        "\n",
        "def orthogonal(w):\n",
        "    \n",
        "    w_kw = K.int_shape(w)[0]\n",
        "    w_kh = K.int_shape(w)[1]\n",
        "    w_w = K.int_shape(w)[2]\n",
        "    w_h = K.int_shape(w)[3]\n",
        "    \n",
        "    temp = 0\n",
        "    for i in range(w_kw):\n",
        "        for j in range(w_kh):\n",
        "            wwt = tf.matmul(tf.transpose(w[i,j]), w[i,j])\n",
        "            mi = K.ones_like(wwt) - K.identity(wwt)\n",
        "            a = wwt * mi\n",
        "            a = tf.matmul(tf.transpose(a), a)\n",
        "            a = a * K.identity(a)\n",
        "            temp += K.sum(a)\n",
        "    return 2e-6 * temp\n",
        "\n",
        "def residual_block(x, dim, ks, init_weight, name):\n",
        "    y = Conv2D(dim, ks, strides=1, padding=\"same\", kernel_initializer=init_weight, kernel_regularizer = orthogonal)(x)\n",
        "    y = SwitchNormalization(axis=-1, name=name+'_0')(y)\n",
        "    y = Activation('relu')(y)   \n",
        "    y = Conv2D(dim, ks, strides=1, padding=\"same\", kernel_initializer=init_weight, kernel_regularizer = orthogonal)(y)\n",
        "    y = SwitchNormalization(axis=-1, name=name+'_1')(y)\n",
        "    return Add()([x,y])\n",
        "    \n",
        "def glu(x):\n",
        "    channel = K.int_shape(x)[-1]\n",
        "    channel = channel//2\n",
        "    a = x[..., :channel]\n",
        "    b = x[..., channel:]\n",
        "    return a * K.sigmoid(b)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8nwbBFySVw"
      },
      "source": [
        "Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6BGhWKFyZBR"
      },
      "source": [
        "import functools\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import LeakyReLU, Activation, Input, Reshape, Flatten, Dense, Multiply\n",
        "from keras.layers import Conv2D, Conv2DTranspose, ZeroPadding2D, Lambda, Concatenate, Add\n",
        "from keras.layers import BatchNormalization, Dropout, Subtract, GlobalAveragePooling2D\n",
        "\n",
        "init_weight = 'he_normal'\n",
        "res_init_weight = 'he_normal'\n",
        "\n",
        "regular = None\n",
        "\n",
        "def tileAttr(x):\n",
        "        x = tf.expand_dims(x, axis = 1)\n",
        "        x = tf.expand_dims(x, axis = 2)\n",
        "        return tf.tile(x, [1, 256, 256, 1])\n",
        "    \n",
        "def tileAttr2(x):\n",
        "        x = tf.expand_dims(x, axis = 1)\n",
        "        x = tf.expand_dims(x, axis = 2)\n",
        "        return tf.tile(x, [1, 4, 4, 1])\n",
        "\n",
        "def generator(img, attr, size):\n",
        "    \n",
        "    concat = Concatenate()([img, Lambda(tileAttr)(attr)])\n",
        "    \n",
        "    DownSample = functools.partial(Conv2D, padding=\"same\" , kernel_initializer=init_weight, kernel_regularizer = orthogonal)\n",
        "    UpSample = functools.partial(Conv2DTranspose, padding=\"same\" , kernel_initializer=init_weight, kernel_regularizer = orthogonal)\n",
        "    \n",
        "    conv_in = DownSample(64, 7, name=\"conv_in_conv\")(concat)\n",
        "    conv_in = SwitchNormalization(axis=-1, name=\"conv_in_norm\")(conv_in)\n",
        "    conv_in = Activation('relu', name=\"conv_in_relu\")(conv_in)\n",
        "    \n",
        "    down1 = DownSample(128, 4, strides=2, name=\"down1_conv\")(conv_in)\n",
        "    down1 = SwitchNormalization(axis=-1, name=\"down1_norm\")(down1)\n",
        "    down1 = Activation('relu', name=\"down1_relu\")(down1)\n",
        "    \n",
        "    down2 = DownSample(256, 4, strides=2, name=\"down2_conv\")(down1)\n",
        "    down2 = SwitchNormalization(axis=-1, name=\"down2_norm\")(down2)\n",
        "    down2 = Activation('relu', name=\"down2_relu\")(down2)\n",
        "    \n",
        "    resb = residual_block(down2, 256, 3, res_init_weight, 'block1')\n",
        "    resb = residual_block(resb, 256, 3, res_init_weight, 'block2')\n",
        "    resb = residual_block(resb, 256, 3, res_init_weight, 'block3')\n",
        "    \n",
        "    encode_out = resb\n",
        "    \n",
        "    resb = residual_block(resb, 256, 3, res_init_weight, 'block4')\n",
        "    resb = residual_block(resb, 256, 3, res_init_weight, 'block5')\n",
        "    resb = residual_block(resb, 256, 3, res_init_weight, 'block6')\n",
        "    \n",
        "    up2 = UpSample(128, 4, strides=2, name=\"up2_deconv2\")(resb)\n",
        "    up2 = SwitchNormalization(axis=-1, name=\"up2_norm\")(up2)\n",
        "    up2 = Activation('relu', name=\"up2_relu\")(up2)\n",
        "    brid2 = up2\n",
        "    \n",
        "    up1 = UpSample(64 , 4, strides=2, name=\"up1_deconv2\")(brid2)\n",
        "    up1 = SwitchNormalization(axis=-1, name=\"up1_norm\")(up1)\n",
        "    up1 = Activation('relu', name=\"up1_relu\")(up1)\n",
        "    brid3 = up1\n",
        "    \n",
        "    conv_out = DownSample(3, 7, name=\"conv_out_conv\")(brid3)\n",
        "    conv_out = Activation('tanh', name=\"conv_out_tanh\")(conv_out)\n",
        "    return conv_out, encode_out\n",
        "\n",
        "def discriminator(imgA, imgB, attr, size, att_size):\n",
        "    \n",
        "    filters = [64, 128, 256, 512, 1024, 2048]\n",
        "    \n",
        "    convs = [Conv2D(64, 4, strides=2, padding='same', kernel_initializer=init_weight, kernel_regularizer=regular, name=\"conv1\"),\n",
        "             Conv2D(128, 4, strides=2, padding='same', kernel_initializer=init_weight, kernel_regularizer=regular, name=\"conv2\"),\n",
        "             Conv2D(256, 4, strides=2, padding='same', kernel_initializer=init_weight, kernel_regularizer=regular, name=\"conv3\"),\n",
        "             Conv2D(512, 4, strides=2, padding='same', kernel_initializer=init_weight, kernel_regularizer=regular, name=\"conv4\"),\n",
        "             Conv2D(1024, 4, strides=2, padding='same', kernel_initializer=init_weight, kernel_regularizer=regular, name=\"conv5\"),\n",
        "             Conv2D(2048, 4, strides=2, padding='same', kernel_initializer=init_weight, kernel_regularizer=regular, name=\"conv6\")]\n",
        "    \n",
        "    \n",
        "    #original image\n",
        "    \n",
        "    y1 = imgA\n",
        "    for i in range(6):\n",
        "        y1 = convs[i](y1)\n",
        "        y1 = LeakyReLU(alpha=0.01)(y1)\n",
        "    \n",
        "    #target image\n",
        "    y2 = imgB\n",
        "    for i in range(6):\n",
        "        y2 = convs[i](y2)\n",
        "        y2 = LeakyReLU(alpha=0.01)(y2)\n",
        "    \n",
        "    d_out1 = Conv2D(1, 1, padding='same', kernel_initializer='lecun_normal', kernel_regularizer=regular)(y2)\n",
        "    \n",
        "    d_out3 = Conv2D(64, 1, padding='same', kernel_initializer='lecun_normal', kernel_regularizer=regular)(y2)\n",
        "    d_out3 = Lambda(lambda x: K.mean(x, axis=[-1]))(d_out3)\n",
        "    \n",
        "    d_out2 = Concatenate()([y1, y2, Lambda(tileAttr2)(attr)])\n",
        "    d_out2 = Conv2D(2048, 1, strides=1, kernel_initializer='lecun_normal', kernel_regularizer=regular)(d_out2)\n",
        "    d_out2 = LeakyReLU(alpha=0.01)(d_out2) # 2 2 2048    \n",
        "    d_out2 = Conv2D(1, 1, padding='same', kernel_initializer='lecun_normal', kernel_regularizer=regular)(d_out2)\n",
        "    \n",
        "    return d_out1, d_out2, d_out3"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx8-YDNM8StJ"
      },
      "source": [
        "Import tensorBoardx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njoFl5dW8Zug",
        "outputId": "de069624-01eb-4559-aa98-d47157b89467"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/0b/a26bbe92667c549d39c40b80c5ddec638fbae9521f04aeef26560e07e504/tensorboardX-2.4-py2.py3-none-any.whl (124kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (57.0.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNpEvr71q7c1"
      },
      "source": [
        "RelGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpW1EZjisBE"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.python.keras.backend import set_session\n",
        "from skimage import io, transform\n",
        "from tensorboardX import SummaryWriter\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "class Relgan():\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.path = args.path\n",
        "        self.lr = args.lr\n",
        "        self.b1 = args.beta1\n",
        "        self.b2 = args.beta2\n",
        "        self.batch = args.batch_size\n",
        "        self.sample = args.sample_size\n",
        "        self.epochs = args.epochs\n",
        "        self.lambda1 = args.lambda1\n",
        "        self.lambda2 = args.lambda2\n",
        "        self.lambda4 = args.lambda4\n",
        "        self.lambda5 = args.lambda5\n",
        "        self.gp_l = args.lambda_gp\n",
        "        self.decay = self.lr / self.epochs\n",
        "        self.imgSize = args.img_size\n",
        "        self.sampleSize = args.img_size\n",
        "        self.vecSize = args.vec_size\n",
        "        self.step = args.step * 200\n",
        "\n",
        "        self.lr -= self.decay * self.step\n",
        "\n",
        "        self.img_shape = (self.imgSize, self.imgSize, 3)\n",
        "        self.vec_shape = (self.vecSize,)\n",
        "\n",
        "        self.get_model()\n",
        "        self.get_loss()\n",
        "        self.get_optimizer()\n",
        "        self.datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "        self.writer = SummaryWriter()\n",
        "\n",
        "    def get_model(self):\n",
        "\n",
        "        self.imgA_input = Input(shape=self.img_shape)\n",
        "        self.imgB_input = Input(shape=self.img_shape)\n",
        "        self.vec_input_pos = Input(shape=self.vec_shape)\n",
        "        self.vec_input_neg = Input(shape=self.vec_shape)\n",
        "\n",
        "        g_out = generator(self.imgA_input, self.vec_input_pos, self.imgSize)\n",
        "\n",
        "        self.g_model = Model(inputs=[self.imgA_input, self.vec_input_pos], outputs=g_out)\n",
        "\n",
        "        d_out = discriminator(self.imgA_input, self.imgB_input, self.vec_input_pos, self.imgSize, self.vecSize)\n",
        "\n",
        "        self.d_model = Model(inputs=[self.imgA_input, self.imgB_input, self.vec_input_pos], \\\n",
        "                             outputs=d_out)\n",
        "\n",
        "        print(self.g_model.summary())\n",
        "        print(self.d_model.summary())\n",
        "\n",
        "        plot_model(self.g_model, to_file='g_model.png')\n",
        "        plot_model(self.d_model, to_file='d_model.png')\n",
        "\n",
        "    def get_loss(self):\n",
        "\n",
        "        def cal_df_gp():\n",
        "\n",
        "            def cal_gp(gradients):\n",
        "                gradients_sqr = K.square(gradients[0])\n",
        "                gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
        "                gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
        "                gradient_penalty = K.mean(K.square(1 - gradient_l2_norm))\n",
        "                return gradient_penalty\n",
        "\n",
        "            alpha = K.random_uniform_variable(shape=(1,), low=0, high=1)\n",
        "\n",
        "            mix_tar = alpha * self.img_a + (1 - alpha) * self.img_a2b\n",
        "\n",
        "            mix_outputs_a2b = self.d_model([self.img_a, mix_tar, self.vec_ab_pos])\n",
        "            mix_outputs_a2ab = self.d_model([self.img_a, self.img_a2ab, self.vec_ab_pos])\n",
        "\n",
        "            gradients_a2b = K.gradients([mix_outputs_a2b[0]], [mix_tar])\n",
        "            gradients_a2ab = K.gradients([mix_outputs_a2ab[2]], [self.img_a2ab])\n",
        "\n",
        "            df_gp = cal_gp(gradients_a2b) + cal_gp(gradients_a2ab)\n",
        "\n",
        "            return df_gp\n",
        "\n",
        "        def lsgan(xs, ts):\n",
        "            real = 0\n",
        "            fake = 0\n",
        "            for i in range(len(xs)):\n",
        "                if ts[i] == 1:\n",
        "                    real += K.mean(K.square(K.ones_like(xs[i]) - xs[i]), axis=[-1])\n",
        "                else:\n",
        "                    fake += K.mean(K.square(K.zeros_like(xs[i]) - xs[i]), axis=[-1])\n",
        "\n",
        "            return real + fake\n",
        "\n",
        "        self.img_a = Input(shape=self.img_shape)\n",
        "        self.img_b = Input(shape=self.img_shape)\n",
        "        self.img_c = Input(shape=self.img_shape)\n",
        "\n",
        "        self.vec_ab_pos = Input(shape=self.vec_shape)\n",
        "        self.vec_ac_pos = Input(shape=self.vec_shape)\n",
        "        self.vec_cb_pos = Input(shape=self.vec_shape)\n",
        "\n",
        "        self.img_a2b, self.enc_a2b = self.g_model([self.img_a, self.vec_ab_pos])\n",
        "        self.img_a2a, self.enc_a2a = self.g_model([self.img_a, K.zeros_like(self.vec_ab_pos)])\n",
        "        self.img_a2b2a, _ = self.g_model([self.img_a2b, -self.vec_ab_pos])\n",
        "\n",
        "        inter_seed = K.random_uniform_variable(shape=([self.batch, ]), low=0, high=1)\n",
        "        inter_seed = K.reshape(inter_seed, [self.batch, 1])\n",
        "        self.img_a2ab, self.enc_a2ab = self.g_model([self.img_a, inter_seed * self.vec_ab_pos])\n",
        "\n",
        "        input_real = [self.img_a, self.img_b, self.vec_ab_pos]\n",
        "        input_fake = [self.img_a, self.img_a2b, self.vec_ab_pos]\n",
        "        input_w_ori = [self.img_c, self.img_b, self.vec_ab_pos]\n",
        "        input_w_tar = [self.img_a, self.img_c, self.vec_ab_pos]\n",
        "        input_w_vec1 = [self.img_a, self.img_b, self.vec_ac_pos]\n",
        "        input_w_vec2 = [self.img_a, self.img_b, self.vec_cb_pos]\n",
        "\n",
        "        input_inter = [self.img_a, self.img_a2ab, inter_seed * self.vec_ab_pos]\n",
        "        input_zero = [self.img_a, self.img_a2a, K.zeros_like(self.vec_ab_pos)]\n",
        "\n",
        "        d_real, dc_real, _ = self.d_model(input_real)\n",
        "\n",
        "        d_fake, dc_fake, di_fake = self.d_model(input_fake)\n",
        "\n",
        "        d_w_ori, dc_w_ori, _ = self.d_model(input_w_ori)\n",
        "        d_w_tar, dc_w_tar, _ = self.d_model(input_w_tar)\n",
        "        d_w_vec1, dc_w_vec1, _ = self.d_model(input_w_vec1)\n",
        "        d_w_vec2, dc_w_vec2, _ = self.d_model(input_w_vec2)\n",
        "\n",
        "        _, _, di_inter = self.d_model(input_inter)\n",
        "        _, _, di_zero = self.d_model(input_zero)\n",
        "\n",
        "        self.df_loss = lsgan([d_real, d_fake], [1, 0])\n",
        "        self.dc_loss = lsgan([dc_real, dc_fake, dc_w_ori, dc_w_tar, dc_w_vec1, dc_w_vec2], [1, 0, 0, 0, 0, 0])\n",
        "        inter_seed_rep = K.flatten(inter_seed)\n",
        "\n",
        "        di_temp = K.switch(K.less(inter_seed_rep, 0.5 * K.ones_like(inter_seed_rep)), di_zero, di_fake)\n",
        "\n",
        "        self.di_loss = K.square(K.minimum(inter_seed_rep, K.ones_like(inter_seed_rep) - inter_seed_rep) * K.ones_like(\n",
        "            di_inter) - di_inter) + K.square(di_temp)\n",
        "        print('self.df_loss', K.int_shape(self.df_loss))\n",
        "        print('self.dc_loss', K.int_shape(self.dc_loss))\n",
        "        print('self.di_loss', K.int_shape(self.di_loss))\n",
        "\n",
        "        self.df_gp = cal_df_gp()\n",
        "\n",
        "        self.d_loss = self.df_loss + self.dc_loss + self.gp_l * self.df_gp + self.lambda5 * self.di_loss\n",
        "\n",
        "        self.gf_loss = lsgan([d_real, d_fake], [0, 1])\n",
        "        self.gc_loss = lsgan([dc_real, dc_fake], [0, 1])\n",
        "        self.gi_loss = K.square(di_inter)\n",
        "\n",
        "        dist_a2b = self.enc_a2b - self.enc_a2a\n",
        "        dist_a2ab = self.enc_a2ab - self.enc_a2a\n",
        "\n",
        "        inter_seed = K.reshape(inter_seed, [self.batch, 1, 1, 1])\n",
        "        self.g_inter_loss = K.mean(K.abs(inter_seed * dist_a2b - dist_a2ab))\n",
        "\n",
        "        g_loss_rec1 = K.mean(K.abs(self.img_a - self.img_a2b2a))\n",
        "        g_loss_rec2 = K.mean(K.abs(self.img_a - self.img_a2a))\n",
        "\n",
        "        print('self.gf_loss', K.int_shape(self.gf_loss))\n",
        "        print('self.gc_loss', K.int_shape(self.gc_loss))\n",
        "        print('self.gi_loss', K.int_shape(self.gi_loss))\n",
        "        print('self.g_loss_rec1', K.int_shape(g_loss_rec1))\n",
        "        print('self.g_loss_rec2', K.int_shape(g_loss_rec2))\n",
        "\n",
        "        self.gr_loss = self.lambda1 * g_loss_rec1 + self.lambda2 * g_loss_rec2\n",
        "        self.g_loss = self.gf_loss + self.gc_loss + self.gr_loss + self.lambda5 * self.gi_loss\n",
        "\n",
        "    def get_optimizer(self):\n",
        "\n",
        "        g_opt = Adam(lr=self.lr, decay=self.decay, beta_1=self.b1, beta_2=self.b2)\n",
        "        g_weights = self.g_model.trainable_weights\n",
        "        g_inputs = [self.img_a, self.img_b, self.vec_ab_pos]\n",
        "\n",
        "        self.g_training_updates = g_opt.get_updates(g_weights, [], self.g_loss)\n",
        "        self.g_train = K.function(g_inputs,\n",
        "                                  [K.mean(self.g_loss),\n",
        "                                   K.mean(self.gf_loss),\n",
        "                                   K.mean(self.gc_loss),\n",
        "                                   K.mean(self.gr_loss),\n",
        "                                   K.mean(self.g_inter_loss),\n",
        "                                   K.mean(self.gi_loss)],\n",
        "                                  self.g_training_updates)\n",
        "\n",
        "        d_opt = Adam(lr=self.lr, decay=self.decay, beta_1=self.b1, beta_2=self.b2)\n",
        "        d_weights = self.d_model.trainable_weights\n",
        "        d_inputs = [self.img_a, self.img_b, self.img_c, self.vec_ab_pos, self.vec_ac_pos, self.vec_cb_pos]\n",
        "\n",
        "        self.d_training_updates = d_opt.get_updates(d_weights, [], self.d_loss)\n",
        "        self.d_train = K.function(d_inputs,\n",
        "                                  [K.mean(self.d_loss),\n",
        "                                   K.mean(self.df_loss),\n",
        "                                   K.mean(self.dc_loss),\n",
        "                                   K.mean(self.gp_l * self.df_gp),\n",
        "                                   K.mean(self.di_loss)],\n",
        "                                  self.d_training_updates)\n",
        "\n",
        "    def get_imgs_tags(self, indexserX, imgIndex, imgAttr):\n",
        "        imgs = [None] * self.batch\n",
        "        atts = [None] * self.batch\n",
        "\n",
        "        for i in range(self.batch):\n",
        "            temp_index = indexserX[i]\n",
        "        #    temp_index = int(re.sub(\"\\D\", \"\", temp_index ))\n",
        "            print(temp_index)\n",
        "            x = np.where(imgIndex == temp_index)\n",
        "            print(x)\n",
        "            print(indexserX[i])                        #\n",
        "            img_fa = imgIndex[int(x[0])]\n",
        "\n",
        "      #      while img_fa == None:\n",
        "       #         temp_index = np.random.choice((imgIndex), 1)[0]    #\n",
        "        #        img_fa = imgIndex[temp_index]\n",
        "            atts[i] = imgAttr[img_fa]\n",
        "            print(temp_index)                                           #\n",
        "            img = io.imread(os.path.join(self.path, str(temp_index).zfill(10)))\n",
        "            imgs[i] = img / 127.5 - 1\n",
        "\n",
        "        imgs = np.array(imgs)\n",
        "        atts = np.array(atts)\n",
        "\n",
        "        self.datagen.fit(imgs)\n",
        "\n",
        "        imgs = self.datagen.flow(imgs, batch_size=self.batch, shuffle=False).next()\n",
        "\n",
        "        return imgs, atts\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        print(\"load index\")\n",
        "        imgIndex = np.load(\"imgIndex.npy\")\n",
        "        imgAttr = np.load(\"anno_dic.npy\").item()\n",
        "        print(\"training\")\n",
        "\n",
        "        ite = self.step\n",
        "\n",
        "        def getIndex():\n",
        "            while True:\n",
        "                count = 0\n",
        "                index_permutation = np.random.permutation((imgIndex))     #\n",
        "                while count + self.batch * 3 < len(imgIndex):\n",
        "                    yield index_permutation[count:(count + self.batch * 3)]\n",
        "                    count = count + self.batch * 3\n",
        "\n",
        "        index_gen = getIndex()\n",
        "\n",
        "        def get_training_data(wrong=False):\n",
        "            indexser = next(index_gen)\n",
        "            indexser1 = indexser[self.batch * 0:self.batch * 1]\n",
        "            indexser2 = indexser[self.batch * 1:self.batch * 2]\n",
        "            indexser3 = indexser[self.batch * 2:self.batch * 3]\n",
        "\n",
        "            img_as, att_as = self.get_imgs_tags(indexser1, imgIndex, imgAttr)\n",
        "            img_bs, att_bs = self.get_imgs_tags(indexser2, imgIndex, imgAttr)\n",
        "            vec_ab_pos = att_bs - att_as\n",
        "\n",
        "            if wrong == False:\n",
        "                return img_as, img_bs, vec_ab_pos\n",
        "\n",
        "            img_cs, att_cs = self.get_imgs_tags(indexser3, imgIndex, imgAttr)\n",
        "\n",
        "            vec_ac_pos = att_cs - att_as\n",
        "            vec_cb_pos = att_bs - att_cs\n",
        "\n",
        "            return img_as, img_bs, img_cs, vec_ab_pos, vec_ac_pos, vec_cb_pos\n",
        "\n",
        "        for ep in range(int(self.epochs)):\n",
        "\n",
        "            t_start = time.time()\n",
        "\n",
        "            img_as, img_bs, img_cs, vec_ab_pos, vec_ac_pos, vec_cb_pos = get_training_data(wrong=True)\n",
        "\n",
        "            for i in range(1):\n",
        "                errD = self.d_train([img_as, img_bs, img_cs, vec_ab_pos, vec_ac_pos, vec_cb_pos])\n",
        "\n",
        "            for i in range(1):\n",
        "                errG = self.g_train([img_as, img_bs, vec_ab_pos])\n",
        "\n",
        "            t_end = time.time()\n",
        "\n",
        "            print(\n",
        "                \"%9.6f %9.6f | real: %7.4f wrong: %7.4f gp: %7.4f| fake: %7.4f wrong: %7.4f recs: %7.4f enc: %7.4f| time: %.4f\" % (\n",
        "                errD[0], errG[0], errD[1], errD[2], errD[3], errG[1], errG[2], errG[3], errG[4], t_end - t_start))\n",
        "\n",
        "            self.writer.add_scalar('d_loss', errD[0], ite)\n",
        "            self.writer.add_scalar('g_loss', errG[0], ite)\n",
        "            self.writer.add_scalar('df_loss', errD[1], ite)\n",
        "            self.writer.add_scalar('gf_loss', errG[1], ite)\n",
        "            self.writer.add_scalar('dc_loss', errD[2], ite)\n",
        "            self.writer.add_scalar('gc_loss', errG[2], ite)\n",
        "            self.writer.add_scalar('gr_loss', errG[3], ite)\n",
        "            self.writer.add_scalar('inter_loss', errG[4], ite)\n",
        "            self.writer.add_scalar('gp_loss', errD[3], ite)\n",
        "            self.writer.add_scalar('gi_loss', errG[5], ite)\n",
        "            self.writer.add_scalar('di_loss', errD[4], ite)\n",
        "\n",
        "            if ite % 50 == 0 and ite > 0:\n",
        "\n",
        "                img_as, img_bs, vec_ab_pos = get_training_data(wrong=False)\n",
        "\n",
        "                g_a2b = [img_as[:self.sample], vec_ab_pos[:self.sample]]\n",
        "                fakea2b, _ = self.g_model.predict(g_a2b)\n",
        "\n",
        "                g_a2a = [img_as[:self.sample], np.zeros([self.sample, self.vecSize])]\n",
        "                fakea2a, _ = self.g_model.predict(g_a2a)\n",
        "\n",
        "                g_a2b2a = [fakea2b[:self.sample], -vec_ab_pos[:self.sample]]\n",
        "                fakea2b2a, _ = self.g_model.predict(g_a2b2a)\n",
        "\n",
        "                images = np.concatenate([img_as[:self.sample], fakea2b, fakea2b2a, fakea2a], axis=0)\n",
        "\n",
        "                width = self.sample\n",
        "                height = 4\n",
        "                new_im = Image.new('RGB', (self.sampleSize * height, self.sampleSize * width))\n",
        "                for ii in range(height):\n",
        "                    for jj in range(width):\n",
        "                        index = ii * width + jj\n",
        "                        image = (images[index] / 2 + 0.5) * 255\n",
        "                        image = transform.resize(image, (self.sampleSize, self.sampleSize), preserve_range=True)\n",
        "                        #                         image = image*255\n",
        "                        image = image.astype(np.uint8)\n",
        "                        new_im.paste(Image.fromarray(image, \"RGB\"), (self.sampleSize * ii, self.sampleSize * jj))\n",
        "                filename = \"img/fakeFace%d.jpg\" % (ite // 200)\n",
        "                new_im.save(filename)\n",
        "\n",
        "                try:\n",
        "                    self.g_model.save(\"model/generator%d.h5\" % (ite // 200))\n",
        "                    self.d_model.save(\"model/discriminator.h5\")\n",
        "                except:\n",
        "                    print('Pass save')\n",
        "            ite = ite + 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hlhjsovrAAq"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBzgOvNWkIkc"
      },
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-p\", \"--path\", type=str, default='E:\\Current_Sem_Project\\CSE465\\Dataset\\celeba-hq\\celeba-256', help=\"data path\")\n",
        "parser.add_argument(\"-d\", \"--device\", type=str, default='0', help=\"gpu device\")\n",
        "parser.add_argument(\"-g\", \"--growth\", type=bool, default=False, help=\"allow_growth\")\n",
        "parser.add_argument(\"-s\", \"--step\", type=int, default=0, help=\"train_step\")\n",
        "parser.add_argument(\"-l\", \"--lr\", type=float, default=5e-5)\n",
        "parser.add_argument(\"-b1\", \"--beta1\", type=float, default=0.5)\n",
        "parser.add_argument(\"-b2\", \"--beta2\", type=float, default=0.999)\n",
        "parser.add_argument(\"-batch\", \"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"-sample\", \"--sample_size\", type=int, default=2)\n",
        "parser.add_argument(\"-ep\", \"--epochs\", type=int, default=400000)\n",
        "parser.add_argument(\"-l1\", \"--lambda1\", type=int, default=10)\n",
        "parser.add_argument(\"-l2\", \"--lambda2\", type=int, default=10)\n",
        "parser.add_argument(\"-l4\", \"--lambda4\", type=int, default=10)\n",
        "parser.add_argument(\"-l5\", \"--lambda5\", type=int, default=10)\n",
        "parser.add_argument(\"-gp\", \"--lambda_gp\", type=int, default=150)\n",
        "parser.add_argument(\"-img\", \"--img_size\", type=int, default=256)\n",
        "parser.add_argument(\"-v\", \"--vec_size\", type=int, default=17)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "\n",
        "from relgan import Relgan\n",
        "\n",
        "# K.set_floatx('float64')\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "set_session(tf.compat.v1.Session(config=config))\n",
        "\n",
        "rel_gan = Relgan(args)\n",
        "rel_gan.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULvx3yyFr2MH"
      },
      "source": [
        "DemoTranslation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaOeBbqpscC_"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras import backend as K\n",
        "from skimage import io, transform\n",
        "from keras.models import load_model\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from contrib.ops import SwitchNormalization\n",
        "from module import *\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-d\", \"--device\", type=str, default='0')\n",
        "args = parser.parse_args()\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "set_session(tf.Session(config=config))\n",
        "\n",
        "def tileAttr(x):\n",
        "    x = tf.expand_dims(x, axis = 1)\n",
        "    x = tf.expand_dims(x, axis = 2)\n",
        "    return tf.tile(x, [1, 256, 256, 1])\n",
        "\n",
        "def tileAttr2(x):\n",
        "        x = tf.expand_dims(x, axis = 1)\n",
        "        x = tf.expand_dims(x, axis = 2)\n",
        "        return tf.tile(x, [1, 4, 4, 1])\n",
        "    \n",
        "# train_path = '/share/diskB/willy/GanExample/FaceAttributeChange_StarGAN/model/generator499.h5'\n",
        "\n",
        "def testPic(img, gender, bangs=-1, glasses=1):\n",
        "    temp3 = io.imread(img)\n",
        "    tempb = transform.resize(temp3, [256,256])\n",
        "    tempb = tempb[:,:,:3]\n",
        "    tempb = tempb*2 - 1\n",
        "    \n",
        "    # imgIndex = np.load(\"imgIndex.npy\")\n",
        "# imgAttr = np.load(\"anno_dic.npy\").item()\n",
        "\n",
        "    new_attrs = ['5_o_Clock_Shadow', 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Male', 'Mustache', 'Pale_Skin', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Hat', 'Young']\n",
        "\n",
        "    def att2vec(attt):\n",
        "        temp_vec = np.expand_dims(attt, axis=0)\n",
        "        return temp_vec\n",
        "\n",
        "    attrs_pos, attrs_neg = [],[]\n",
        "    attr = []\n",
        "    \n",
        "    temp = np.zeros([17])\n",
        "    # temp[new_attrs.index('Brown_Hair')] = -1\n",
        "    temp[new_attrs.index('Black_Hair')] = -1\n",
        "    temp[new_attrs.index('Blond_Hair')] = 1\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    temp[new_attrs.index('Black_Hair')] = -1\n",
        "    # temp[new_attrs.index('Brown_Hair')] = -1\n",
        "    # temp[new_attrs.index('Black_Hair')] = 1\n",
        "    temp[new_attrs.index('Brown_Hair')] = 1\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    if gender==1:\n",
        "        temp[new_attrs.index('Male')] = -1\n",
        "    else:\n",
        "        temp[new_attrs.index('Male')] = 1\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    #if gender==0:\n",
        "    temp[new_attrs.index('Male')] = 1 \n",
        "    temp[new_attrs.index('Goatee')] = 1\n",
        "    temp[new_attrs.index('Mustache')] = 1\n",
        "    temp[new_attrs.index('5_o_Clock_Shadow')] = 1\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    temp[new_attrs.index('Pale_Skin')] = 1\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    temp[new_attrs.index('Smiling')] = 1\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    temp[new_attrs.index('Bangs')] = bangs\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    temp[new_attrs.index('Eyeglasses')] = glasses\n",
        "    attr.append(temp)\n",
        "\n",
        "    temp = np.zeros([17])\n",
        "    temp[new_attrs.index('Gray_Hair')] = 1\n",
        "    temp[new_attrs.index('Young')] = -1\n",
        "    attr.append(temp)\n",
        "    \n",
        "    for at in attr:\n",
        "        att_pos = att2vec(at)\n",
        "        attrs_pos.append(att_pos)\n",
        "\n",
        "    attrs_pos = np.concatenate(attrs_pos, axis=0)    \n",
        "    \n",
        "    output = np.expand_dims(tempb, axis=0)\n",
        "    output2 = np.tile(output, [len(attr), 1, 1, 1])\n",
        "    outputs_, _ = relGan.predict([output2, attrs_pos])\n",
        "    images = np.concatenate([output, outputs_], axis = 0)\n",
        "    width = 1\n",
        "    height = len(attr) + 1\n",
        "    new_im = Image.new('RGB', (256*height, 256*width))\n",
        "    for ii in range(height):\n",
        "        for jj in range(width):\n",
        "            index=ii*width+jj\n",
        "            image = (images[index]/2+0.5)*255\n",
        "            image = image.astype(np.uint8)\n",
        "            new_im.paste(Image.fromarray(image,\"RGB\"), (256*ii,256*jj))\n",
        "    ans = np.array(new_im)\n",
        "    return ans\n",
        "\n",
        "# temp3 = io.imread('/share/data/celeba-hq/celeba-256/12345.jpg')\n",
        "# version = int(sys.argv[2])\n",
        "# if version==-1:\n",
        "#     version = len(os.listdir('img'))-2\n",
        "version = 519\n",
        "\n",
        "print(version)\n",
        "\n",
        "train_path = './generator'+str(version)+'.h5'\n",
        "#train_path = 'model/generator1511.h5'\n",
        "\n",
        "# train_path = 'good_v0513.h5'\n",
        "\n",
        "def orthogonal(w):\n",
        "    \n",
        "    w_kw = K.int_shape(w)[0]\n",
        "    w_kh = K.int_shape(w)[1]\n",
        "    w_w = K.int_shape(w)[2]\n",
        "    w_h = K.int_shape(w)[3]\n",
        "    \n",
        "    temp = 0\n",
        "    for i in range(w_kw):\n",
        "        for j in range(w_kh):\n",
        "            wwt = tf.matmul(tf.transpose(w[i,j]), w[i,j])\n",
        "            mi = K.ones_like(wwt) - K.identity(wwt)\n",
        "            a = wwt * mi\n",
        "            a = tf.matmul(tf.transpose(a), a)\n",
        "            a = a * K.identity(a)\n",
        "            temp += K.sum(a)\n",
        "    return 1e-4 * temp\n",
        "\n",
        "img_shape = (256, 256, 3)\n",
        "vec_shape = (17,)\n",
        "\n",
        "imgA_input = Input(shape=img_shape)\n",
        "imgB_input = Input(shape=img_shape)\n",
        "vec_input_pos = Input(shape=vec_shape)\n",
        "vec_input_neg = Input(shape=vec_shape)\n",
        "\n",
        "g_out = generator(imgA_input, vec_input_pos, 256)\n",
        "relGan = Model(inputs=[imgA_input, vec_input_pos], outputs=g_out)\n",
        "relGan.load_weights(train_path)\n",
        "relGan.summary()\n",
        "\n",
        "lengh = 10\n",
        "temp = [None]*lengh\n",
        "temp[0] = testPic('test_img/j.png',0)\n",
        "temp[1] = testPic('test_img/c.2.jpg',0)\n",
        "temp[2] = testPic('test_img/es.png',1)\n",
        "temp[3] = testPic('test_img/e.2.png',1)\n",
        "temp[4] = testPic('test_img/g.2.png',1)\n",
        "temp[5] = testPic('test_img/y3.png',1)\n",
        "temp[6] = testPic('test_img/f1.png',1,glasses=-1)\n",
        "temp[7] = testPic('test_img/j1.png',0,glasses=-1)\n",
        "temp[8] = testPic('test_img/c3.png',0)\n",
        "temp[9] = testPic('test_img/g3.png',1,glasses=-1)\n",
        "\n",
        "new_im = Image.new('RGB', (256*10, 256*lengh))\n",
        "for jj in tqdm(range(lengh)):\n",
        "    index = jj\n",
        "    image = temp[index]\n",
        "    new_im.paste(Image.fromarray(image,\"RGB\"), (0,256*jj))\n",
        "new_im.save('test_v%04d.jpg'%version)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx2Z1xu9uYnh"
      },
      "source": [
        "Demo Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOiiLj0LueXg"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from keras import backend as K\n",
        "from skimage import io, transform\n",
        "from keras.models import load_model\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from contrib.ops import SwitchNormalization\n",
        "from module import *\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-d\", \"--device\", type=str, default='0')\n",
        "parser.add_argument(\"-f\", \"--file\", type=str, default='test_img/y3.png')\n",
        "parser.add_argument(\"-o\", \"--output\", type=str, default='output.gif')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "set_session(tf.Session(config=config))\n",
        "\n",
        "def tileAttr(x):\n",
        "    x = tf.expand_dims(x, axis = 1)\n",
        "    x = tf.expand_dims(x, axis = 2)\n",
        "    return tf.tile(x, [1, 256, 256, 1])\n",
        "\n",
        "def tileAttr2(x):\n",
        "        x = tf.expand_dims(x, axis = 1)\n",
        "        x = tf.expand_dims(x, axis = 2)\n",
        "        return tf.tile(x, [1, 4, 4, 1])\n",
        "\n",
        "# train_path = '/share/diskB/willy/GanExample/FaceAttributeChange_StarGAN/model/generator499.h5'\n",
        "\n",
        "def testPic(img, gender, bangs=-1, glasses=1):\n",
        "    temp3 = io.imread(img)\n",
        "    tempb = transform.resize(temp3, [256,256])\n",
        "    tempb = tempb[:,:,:3]\n",
        "    tempb = tempb*2 - 1\n",
        "    \n",
        "    # imgIndex = np.load(\"imgIndex.npy\")\n",
        "# imgAttr = np.load(\"anno_dic.npy\").item()\n",
        "\n",
        "    new_attrs = ['5_o_Clock_Shadow', 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Male', 'Mustache', 'Pale_Skin', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Hat', 'Young']\n",
        "\n",
        "    def att2vec(attt):\n",
        "        temp_vec = np.expand_dims(attt, axis=0)\n",
        "        return temp_vec\n",
        "\n",
        "    attrs_pos, attrs_neg = [],[]\n",
        "    attr = []\n",
        " \n",
        "    for i in range(0,21):\n",
        "        temp = np.zeros([17])\n",
        "        \n",
        "        temp[new_attrs.index('Smiling')] = 0.05 * i\n",
        "        #temp[new_attrs.index('Gray_Hair')] = 0.07* i\n",
        "        #temp[new_attrs.index('Young')] = -0.07* i\n",
        "        attr.append(temp)\n",
        "        \n",
        "    for i in range(21,0,-1):\n",
        "        temp = np.zeros([17])\n",
        "        \n",
        "        temp[new_attrs.index('Smiling')] = 0.05 * i\n",
        "        #temp[new_attrs.index('Gray_Hair')] = 0.07* i\n",
        "        #temp[new_attrs.index('Young')] = -0.07* i\n",
        "        attr.append(temp)\n",
        "        \n",
        "    for at in attr:\n",
        "        att_pos = att2vec(at)\n",
        "        attrs_pos.append(att_pos)\n",
        "\n",
        "    attrs_pos = np.concatenate(attrs_pos, axis=0)    \n",
        "    \n",
        "    output = np.expand_dims(tempb, axis=0)\n",
        "    output2 = np.tile(output, [len(attr), 1, 1, 1])\n",
        "    outputs_, _ = relGan.predict([output2, attrs_pos])\n",
        "    outputs_ = np.ndarray.astype((outputs_/2+0.5)*255, np.uint8)\n",
        "    imageio.mimsave('simple_'+args.output, outputs_[10:31])\n",
        "    imageio.mimsave(args.output, outputs_)\n",
        "\n",
        "# temp3 = io.imread('/share/data/celeba-hq/celeba-256/12345.jpg')\n",
        "version = 519\n",
        "#version = 461\n",
        "\n",
        "train_path = './generator'+str(version)+'.h5'\n",
        "\n",
        "print('version: ', version)\n",
        "#train_path = 'model/generator1511.h5'\n",
        "\n",
        "# train_path = 'good_v0513.h5'\n",
        "\n",
        "img_shape = (256, 256, 3)\n",
        "vec_shape = (17,)\n",
        "\n",
        "imgA_input = Input(shape=img_shape)\n",
        "imgB_input = Input(shape=img_shape)\n",
        "vec_input_pos = Input(shape=vec_shape)\n",
        "vec_input_neg = Input(shape=vec_shape)\n",
        "\n",
        "g_out = generator(imgA_input, vec_input_pos, 256)\n",
        "relGan = Model(inputs=[imgA_input, vec_input_pos], outputs=g_out)\n",
        "relGan.load_weights(train_path)\n",
        "relGan.summary()\n",
        "\n",
        "lengh = 1\n",
        "temp = [None]*lengh\n",
        "temp[0] = testPic(args.file, 1, glasses=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}